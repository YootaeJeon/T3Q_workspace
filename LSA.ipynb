{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM의 크기(shape) : (4, 9)\n"
     ]
    }
   ],
   "source": [
    "A = np.array(\n",
    "[[0,0,0,1,0,1,1,0,0],\n",
    " [0,0,0,1,1,0,1,0,0],\n",
    " [0,1,1,0,2,0,0,0,0],\n",
    " [1,0,0,0,0,0,0,1,1]])\n",
    "\n",
    "print('DTM의 크기(shape) :', np.shape(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n",
      "행렬 U의 크기(shape) :\n",
      " (4, 4)\n",
      "대각행렬 s:(내림차순)\n",
      " [2.68731789 2.04508425 1.73205081 0.77197992]\n",
      "특이값 벡터 :\n",
      "[2.69 2.05 1.73 0.77]\n",
      "특이값 벡터의 크기(shape) : (4,)\n"
     ]
    }
   ],
   "source": [
    "# SVD란 A가 m × n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것\n",
    "\n",
    "U, s, VT = np.linalg.svd(A, full_matrices = True)       #선형대수 함수 (Linear Algebra)_svd 특이값 분해 \n",
    "\n",
    "print('행렬 U :')\n",
    "print(U.round(2))\n",
    "print('행렬 U의 크기(shape) :\\n',np.shape(U))\n",
    "\n",
    "print(\"대각행렬 s:(내림차순)\\n\", s)\n",
    "\n",
    "print('특이값 벡터 :')\n",
    "print(s.round(2))\n",
    "print('특이값 벡터의 크기(shape) :',np.shape(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "대각 행렬 S :\n",
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n",
      "대각 행렬의 크기(shape) :\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성\n",
    "S = np.zeros((4, 9))\n",
    "print(S)\n",
    "print(\"\")\n",
    "print(S[:4, :4])\n",
    "\n",
    "# 특이값을 대각행렬에 삽입\n",
    "S[:4, :4] = np.diag(s)             # 2차원행렬 대각행렬함수 추출\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))\n",
    "\n",
    "print('대각 행렬의 크기(shape) :')\n",
    "print(np.shape(S))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 × 9의 크기를 가지는 대각 행렬 S가 생성되었습니다. 2.69 > 2.05 > 1.73 > 0.77 순으로 값이 내림차순을 보이는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n",
      "행렬 U의 크기(shape) :\n",
      " (4, 4)\n",
      "대각행렬 s:(내림차순)\n",
      " [2.68731789 2.04508425 1.73205081 0.77197992]\n",
      "[2.68731789 2.04508425 1.73205081 0.77197992]\n",
      "대각 행렬 S :\n",
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n",
      "특이값 벡터 :\n",
      "[2.69 2.05 1.73 0.77]\n",
      "특이값 벡터의 크기(shape) : (4,)\n",
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n",
      "직교 행렬 VT의 크기(shape) :\n",
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "# SVD란 A가 m × n 행렬일 때, 다음과 같이 3개의 행렬의 곱으로 분해(decomposition)하는 것\n",
    "\n",
    "print('행렬 U :')\n",
    "print(U.round(2))\n",
    "print('행렬 U의 크기(shape) :\\n',np.shape(U))\n",
    "\n",
    "print(\"대각행렬 s:(내림차순)\\n\", s)\n",
    "print(s)\n",
    "# 특이값을 대각행렬에 삽입\n",
    "S[:4, :4] = np.diag(s)             # 2차원행렬 대각행렬함수 추출\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))\n",
    "\n",
    "print('특이값 벡터 :')\n",
    "print(s.round(2))\n",
    "print('특이값 벡터의 크기(shape) :',np.shape(s))\n",
    "\n",
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))\n",
    "\n",
    "print('직교 행렬 VT의 크기(shape) :')\n",
    "print(np.shape(VT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 × 9의 크기를 가지는 직교 행렬 VT(V의 전치 행렬)가 생성되었습니다. \n",
    "\n",
    "즉, U × S × VT를 하면 기존의 행렬 A가 나와야 합니다. \n",
    "\n",
    "Numpy의 allclose()는 2개의 행렬이 동일하면 True를 리턴합니다. \n",
    "\n",
    "이를 사용하여 정말로 기존의 행렬 A와 동일한지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, np.dot(np.dot(U,S), VT).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "절단된 svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대각 행렬 S :\n",
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 특이값 상위 2개만 보존\n",
    "S = S[:2,:2]\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75]\n",
      " [-0.51  0.44]\n",
      " [-0.83 -0.49]\n",
      " [-0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "U = U[:,:2]\n",
    "print('행렬 U :')\n",
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "VT = VT[:2,:]\n",
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.   -0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "A_prime = np.dot(np.dot(U,S), VT)\n",
    "print(A)\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실습 뉴스 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\envs\\t3q\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\admin\\anaconda3\\envs\\t3q\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\admin\\anaconda3\\envs\\t3q\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\admin\\anaconda3\\envs\\t3q\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\envs\\t3q\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.5.2\n",
      "  latest version: 23.7.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.7.2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import sklearn\n",
    "sklearn.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 수 : 11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('샘플의 수 :',len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스그룹 데이터에는 특수문자가 포함된 다수의 영어문장으로 구성되어져 있습니다. \n",
    "\n",
    "이런 형식의 샘플이 총 11,314개 존재합니다. \n",
    "\n",
    "사이킷런이 제공하는 뉴스그룹 데이터에서 target_name에는 본래 이 뉴스그룹 데이터가 \n",
    "\n",
    "어떤 20개의 카테고리를 갖고있었는지가 저장되어져 있습니다. 이를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join( [w for w in x.split() if len(w)>3 ]))\n",
    "# 전체 단어에 대한 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah, expect people read faq, etc. actually accept hard atheism? need little leap faith, jimmy. your logic runs steam! jim, sorry can't pity you, jim. sorry that have these feelings denial about faith need well, just pretend that will happily ever after anyway. maybe start newsgroup, alt.atheist.hard, won't bummin' much? bye-bye, jim. don't forget your flintstone's chewables! bake timmons,\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 특수문자가 제거되었으며, if나 you와 같은 길이가 3이하인 단어가 제거된 것을 확인할 수 있습니다. \n",
    "\n",
    "뿐만 아니라 대문자가 전부 소문자로 바뀌었습니다. \n",
    "\n",
    "이제 뉴스그룹 데이터에서 불용어를 제거합니다. \n",
    "\n",
    "불용어를 제거하기 위해서 토큰화를 우선 수행합니다. \n",
    "\n",
    "토큰화와 불용어 제거를 순차적으로 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK로부터 불용어를 받아온다.\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# 불용어를 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yeah,', 'expect', 'people', 'read', 'faq,', 'etc.', 'actually', 'accept', 'hard', 'atheism?', 'need', 'little', 'leap', 'faith,', 'jimmy.', 'logic', 'runs', 'steam!', 'jim,', 'sorry', \"can't\", 'pity', 'you,', 'jim.', 'sorry', 'feelings', 'denial', 'faith', 'need', 'well,', 'pretend', 'happily', 'ever', 'anyway.', 'maybe', 'start', 'newsgroup,', 'alt.atheist.hard,', \"bummin'\", 'much?', 'bye-bye,', 'jim.', 'forget', \"flintstone's\", 'chewables!', 'bake', 'timmons,']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_doc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 있었던 불용어에 속하던 your, about, just, that, will, after 단어들이 사라졌을 뿐만 아니라, 토큰화가 수행된 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) TF-IDF 행렬 만들기**\n",
    "\n",
    "불용어 제거를 위해 토큰화 작업을 수행하였지만, TfidfVectorizer(TF-IDF 실습 참고)는 기본적으로 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용합니다. 그렇기 때문에 TfidfVectorizer를 사용해서 TF-IDF 행렬을 만들기 위해서 다시 토큰화 작업을 역으로 취소하는 작업을 수행해보도록 하겠습니다. 이를 역토큰화(Detokenization)라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역토큰화 (토큰화 작업을 역으로 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah, expect people read faq, etc. actually accept hard atheism? need little leap faith, jimmy. logic runs steam! jim, sorry can't pity you, jim. sorry feelings denial faith need well, pretend happily ever anyway. maybe start newsgroup, alt.atheist.hard, bummin' much? bye-bye, jim. forget flintstone's chewables! bake timmons,\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정상적으로 불용어가 제거된 상태에서 역토큰화가 수행되었음을 확인할 수 있습니다.\n",
    "\n",
    "이제 사이킷런의 TfidfVectorizer를 통해 단어 1,000개에 대한 TF-IDF 행렬을 만들 것입니다.\n",
    "\n",
    " 물론 텍스트 데이터에 있는 모든 단어를 가지고 행렬을 만들 수는 있겠지만, 여기서는 1,000개의 단어로 제한하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 행렬의 크기 : (11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, # 상위 1,000개의 단어를 보존\n",
    "max_df = 0.5, smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "\n",
    "# TF-IDF 행렬의 크기 확인\n",
    "print('TF-IDF 행렬의 크기 :',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 687)\t0.1296353669652296\n",
      "  (0, 474)\t0.1654349645287957\n",
      "  (0, 528)\t0.12056611615981094\n",
      "  (0, 129)\t0.1353917589343785\n",
      "  (0, 546)\t0.13730094657898326\n",
      "  (0, 406)\t0.13187765371261517\n",
      "  (0, 734)\t0.16560097932778423\n",
      "  (0, 824)\t0.1916531368961578\n",
      "  (0, 750)\t0.17005059429692632\n",
      "  (0, 211)\t0.16429581785105885\n",
      "  (0, 748)\t0.1678393395128178\n",
      "  (0, 731)\t0.1334001116346078\n",
      "  (0, 894)\t0.09142875717846821\n",
      "  (0, 506)\t0.1747572305861188\n",
      "  (0, 986)\t0.12651035082406945\n",
      "  (0, 469)\t0.35795726654853505\n",
      "  (0, 563)\t0.6872510148195169\n",
      "  (0, 848)\t0.1587201954825918\n",
      "  (0, 854)\t0.163349948814478\n",
      "  (0, 867)\t0.11600928494264434\n",
      "  (1, 80)\t0.22175666279878037\n",
      "  (1, 610)\t0.2077273970865982\n",
      "  (1, 844)\t0.16953640462205893\n",
      "  (1, 559)\t0.16291087962121\n",
      "  (1, 828)\t0.37072745091156617\n",
      "  :\t:\n",
      "  (11313, 219)\t0.12164437858388767\n",
      "  (11313, 240)\t0.1579505998623282\n",
      "  (11313, 909)\t0.16851830379964058\n",
      "  (11313, 155)\t0.14375875155695303\n",
      "  (11313, 943)\t0.14124328059721142\n",
      "  (11313, 547)\t0.14358487860740204\n",
      "  (11313, 714)\t0.1383126045098466\n",
      "  (11313, 931)\t0.10575170241278291\n",
      "  (11313, 996)\t0.22939729134560471\n",
      "  (11313, 746)\t0.1365189123022372\n",
      "  (11313, 733)\t0.16288639499202168\n",
      "  (11313, 831)\t0.15918615786694124\n",
      "  (11313, 618)\t0.1539018567625116\n",
      "  (11313, 191)\t0.14648466456171155\n",
      "  (11313, 500)\t0.16618223968497295\n",
      "  (11313, 404)\t0.2903490969811753\n",
      "  (11313, 763)\t0.10750982572518213\n",
      "  (11313, 995)\t0.11962894175537747\n",
      "  (11313, 173)\t0.14206039356514136\n",
      "  (11313, 188)\t0.13997705927761914\n",
      "  (11313, 950)\t0.10545422037488464\n",
      "  (11313, 486)\t0.08478369120527522\n",
      "  (11313, 512)\t0.08117791193060786\n",
      "  (11313, 66)\t0.12407739790473847\n",
      "  (11313, 345)\t0.15624472056450472\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4) 토픽 모델링(Topic Modeling)**\n",
    "\n",
    "이제 TF-IDF 행렬을 다수의 행렬로 분해해보도록 하겠습니다. 여기서는 사이킷런의 절단된 SVD(Truncated SVD)를 사용합니다. 절단된 SVD를 사용하면 차원을 축소할 수 있습니다. 원래 기존 뉴스그룹 데이터가 20개의 카테고리를 갖고있었기 때문에, 20개의 토픽을 가졌다고 가정하고 토픽 모델링을 시도해보겠습니다. 토픽의 숫자는 n_components의 파라미터로 지정이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(svd_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 svd_model.componets_는 앞서 배운 LSA에서 VT에 해당됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확하게 토픽의 수 t × 단어의 수의 크기를 가지는 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '000' '01' '02' '04' '0d' '0t' '10' '100' '11' '12' '13' '14' '145'\n",
      " '15' '16' '17' '18' '19' '1990' '1991' '1992' '1993' '1d9' '1t' '20' '21'\n",
      " '22' '23' '24' '25' '27' '2di' '2tm' '30' '32' '34' '34u' '3t' '40' '45'\n",
      " '50' '500' '55' '6ei' '6um' '75' '75u' '7ey' '7u' '80' '800' '90' '91'\n",
      " '92' '93' '9v' 'a86' 'ability' 'able' 'ac' 'accept' 'access' 'according'\n",
      " 'account' 'action' 'actually' 'added' 'addition' 'address'\n",
      " 'administration' 'advance' 'ago' 'agree' 'ah' 'air' 'algorithm' 'allow'\n",
      " 'allowed' 'allows' 'alt' 'amendment' 'america' 'american' 'americans'\n",
      " 'analysis' 'anonymous' 'answer' 'answers' 'anti' 'anybody' 'apparently'\n",
      " 'appear' 'appears' 'apple' 'application' 'applications' 'apply'\n",
      " 'appreciate' 'appreciated' 'approach' 'appropriate' 'april' 'arab'\n",
      " 'archive' 'area' 'areas' 'argument' 'arguments' 'armenia' 'armenian'\n",
      " 'armenians' 'arms' 'army' 'article' 'articles' 'asked' 'asking' 'assume'\n",
      " 'atheism' 'atheists' 'attack' 'attempt' 'au' 'author' 'authority'\n",
      " 'available' 'average' 'avoid' 'away' 'ax' 'b8f' 'base' 'baseball' 'based'\n",
      " 'basic' 'basically' 'basis' 'belief' 'believe' 'best' 'better' 'bh' 'bhj'\n",
      " 'bible' 'bike' 'bios' 'bit' 'bits' 'bj' 'black' 'block' 'blood' 'board'\n",
      " 'body' 'book' 'books' 'bought' 'break' 'bring' 'brought' 'btw' 'build'\n",
      " 'building' 'built' 'business' 'bxn' 'c_' 'ca' 'cable' 'california'\n",
      " 'called' 'calls' 'came' 'canada' 'car' 'card' 'cards' 'care' 'carry'\n",
      " 'cars' 'case' 'cases' 'cause' 'center' 'certain' 'certainly' 'chance'\n",
      " 'change' 'changed' 'changes' 'check' 'chicago' 'child' 'children' 'chip'\n",
      " 'chips' 'choice' 'christ' 'christian' 'christianity' 'christians'\n",
      " 'church' 'chz' 'citizens' 'city' 'civil' 'claim' 'claims' 'class' 'clear'\n",
      " 'clearly' 'clinton' 'clipper' 'clock' 'close' 'code' 'color' 'com' 'come'\n",
      " 'comes' 'coming' 'command' 'comment' 'comments' 'commercial' 'committee'\n",
      " 'common' 'communications' 'community' 'comp' 'companies' 'company'\n",
      " 'complete' 'completely' 'computer' 'condition' 'conference' 'congress'\n",
      " 'consider' 'considered' 'contact' 'contains' 'context' 'continue'\n",
      " 'control' 'controller' 'copy' 'correct' 'cost' 'costs' 'count'\n",
      " 'countries' 'country' 'couple' 'course' 'court' 'cover' 'create'\n",
      " 'created' 'crime' 'cross' 'cs' 'current' 'currently' 'cx' 'd9' 'data'\n",
      " 'date' 'dave' 'david' 'day' 'days' 'dead' 'deal' 'death' 'decided'\n",
      " 'defense' 'define' 'deleted' 'department' 'design' 'designed' 'details'\n",
      " 'developed' 'development' 'device' 'devices' 'did' 'died' 'difference'\n",
      " 'different' 'difficult' 'digital' 'directly' 'directory' 'discussion'\n",
      " 'disease' 'disk' 'disks' 'display' 'distribution' 'division' 'doctor'\n",
      " 'door' 'dos' 'double' 'doubt' 'drive' 'driver' 'drivers' 'drives' 'drug'\n",
      " 'earlier' 'early' 'earth' 'easily' 'east' 'easy' 'edu' 'education'\n",
      " 'effect' 'effective' 'electronic' 'email' 'encryption' 'end'\n",
      " 'enforcement' 'engine' 'entire' 'entry' 'environment' 'equipment' 'error'\n",
      " 'escrow' 'especially' 'event' 'events' 'evidence' 'exactly' 'example'\n",
      " 'excellent' 'exist' 'existence' 'exists' 'expect' 'experience' 'explain'\n",
      " 'export' 'extra' 'face' 'fact' 'faith' 'false' 'family' 'faq' 'fast'\n",
      " 'faster' 'father' 'features' 'federal' 'feel' 'field' 'figure' 'file'\n",
      " 'files' 'final' 'finally' 'fine' 'firearms' 'floppy' 'folks' 'follow'\n",
      " 'following' 'food' 'force' 'form' 'format' 'free' 'freedom' 'friend'\n",
      " 'ftp' 'function' 'functions' 'future' 'g9v' 'game' 'games' 'gave'\n",
      " 'general' 'generally' 'genocide' 'gets' 'getting' 'given' 'gives'\n",
      " 'giving' 'giz' 'gk' 'goal' 'god' 'goes' 'going' 'gone' 'good' 'gov'\n",
      " 'government' 'graphics' 'great' 'greek' 'ground' 'group' 'groups' 'guess'\n",
      " 'gun' 'guns' 'half' 'hand' 'happen' 'happened' 'happens' 'happy' 'hard'\n",
      " 'hardware' 'head' 'health' 'hear' 'heard' 'held' 'hell' 'help' 'high'\n",
      " 'higher' 'history' 'hockey' 'hold' 'holy' 'home' 'hope' 'hours' 'house'\n",
      " 'human' 'idea' 'ideas' 'image' 'images' 'imagine' 'important' 'include'\n",
      " 'included' 'includes' 'including' 'increase' 'individual' 'info'\n",
      " 'information' 'input' 'inside' 'installed' 'instead' 'insurance'\n",
      " 'interested' 'interesting' 'interface' 'internal' 'international'\n",
      " 'internet' 'involved' 'israel' 'israeli' 'issue' 'issues' 'jesus'\n",
      " 'jewish' 'jews' 'jobs' 'john' 'jpeg' 'kept' 'key' 'keyboard' 'keys'\n",
      " 'kill' 'killed' 'kind' 'knew' 'know' 'knowledge' 'known' 'knows' 'lack'\n",
      " 'land' 'language' 'large' 'late' 'later' 'launch' 'law' 'laws' 'lead'\n",
      " 'league' 'learn' 'leave' 'left' 'legal' 'let' 'letter' 'level' 'lib'\n",
      " 'library' 'life' 'light' 'like' 'likely' 'limited' 'line' 'lines' 'list'\n",
      " 'little' 'live' 'lives' 'living' 'lk' 'll' 'local' 'logic' 'long'\n",
      " 'longer' 'look' 'looked' 'looking' 'looks' 'lord' 'lost' 'lots' 'love'\n",
      " 'lower' 'mac' 'machine' 'machines' 'mail' 'mailing' 'main' 'major'\n",
      " 'majority' 'make' 'makes' 'making' 'man' 'manager' 'manual' 'march'\n",
      " 'mark' 'market' 'mass' 'master' 'material' 'matter' 'max' 'maybe' 'mean'\n",
      " 'meaning' 'means' 'media' 'medical' 'member' 'members' 'memory' 'mention'\n",
      " 'mentioned' 'message' 'method' 'middle' 'mike' 'miles' 'military'\n",
      " 'million' 'mind' 'minutes' 'mission' 'mit' 'mode' 'model' 'modem'\n",
      " 'modern' 'money' 'monitor' 'month' 'months' 'moon' 'moral' 'mother'\n",
      " 'motif' 'mouse' 'mq' 'multiple' 'mv' 'names' 'nasa' 'national' 'nature'\n",
      " 'near' 'necessary' 'need' 'needed' 'needs' 'net' 'network' 'new' 'news'\n",
      " 'newsgroup' 'nice' 'night' 'non' 'normal' 'north' 'note' 'number'\n",
      " 'numbers' 'object' 'obvious' 'obviously' 'offer' 'office' 'official'\n",
      " 'old' 'ones' 'open' 'operation' 'opinion' 'opinions' 'option' 'options'\n",
      " 'orbit' 'order' 'org' 'organization' 'original' 'os' 'output' 'outside'\n",
      " 'package' 'page' 'pain' 'paper' 'particular' 'parts' 'party' 'pass'\n",
      " 'past' 'paul' 'pc' 'peace' 'people' 'perfect' 'performance' 'period'\n",
      " 'person' 'personal' 'phone' 'physical' 'pick' 'picture' 'pittsburgh' 'pl'\n",
      " 'place' 'places' 'plan' 'play' 'played' 'player' 'players' 'plus' 'point'\n",
      " 'points' 'police' 'policy' 'political' 'population' 'port' 'position'\n",
      " 'possible' 'possibly' 'post' 'posted' 'posting' 'posts' 'power' 'present'\n",
      " 'president' 'press' 'pretty' 'prevent' 'previous' 'price' 'printer'\n",
      " 'privacy' 'private' 'probably' 'problem' 'problems' 'process' 'product'\n",
      " 'products' 'program' 'programming' 'programs' 'project' 'protect' 'prove'\n",
      " 'provide' 'provided' 'provides' 'pub' 'public' 'published' 'purpose' 'qq'\n",
      " 'quality' 'question' 'questions' 'quite' 'quote' 'radio' 'range' 'rate'\n",
      " 'read' 'reading' 'real' 'reality' 'really' 'reason' 'reasonable'\n",
      " 'reasons' 'received' 'recent' 'recently' 'record' 'reference'\n",
      " 'references' 'regarding' 'regular' 'related' 'release' 'religion'\n",
      " 'religious' 'remember' 'reply' 'report' 'reported' 'reports' 'request'\n",
      " 'require' 'required' 'requires' 'research' 'resource' 'resources'\n",
      " 'response' 'rest' 'result' 'results' 'return' 'right' 'rights' 'road'\n",
      " 'room' 'round' 'rule' 'rules' 'running' 'runs' 'russian' 'safety' 'said'\n",
      " 'sale' 'satellite' 'save' 'say' 'saying' 'says' 'school' 'sci' 'science'\n",
      " 'scientific' 'screen' 'scsi' 'search' 'season' 'second' 'secret'\n",
      " 'section' 'secure' 'security' 'seen' 'self' 'sell' 'send' 'sense' 'sent'\n",
      " 'serial' 'series' 'server' 'service' 'services' 'shall' 'shipping'\n",
      " 'short' 'shot' 'shows' 'shuttle' 'similar' 'simple' 'simply' 'single'\n",
      " 'site' 'sites' 'situation' 'size' 'sl' 'small' 'smith' 'society'\n",
      " 'software' 'soldiers' 'solution' 'somebody' 'soon' 'sorry' 'sort' 'sound'\n",
      " 'sounds' 'source' 'sources' 'south' 'soviet' 'space' 'special' 'specific'\n",
      " 'speed' 'spirit' 'stand' 'standard' 'standards' 'start' 'started' 'state'\n",
      " 'stated' 'statement' 'states' 'station' 'stephanopoulos' 'steve' 'stop'\n",
      " 'story' 'stream' 'street' 'strong' 'study' 'stuff' 'subject' 'suggest'\n",
      " 'sun' 'supply' 'support' 'supports' 'supposed' 'sure' 'surface' 'suspect'\n",
      " 'switch' 'systems' 'table' 'taken' 'takes' 'taking' 'talk' 'talking'\n",
      " 'tape' 'tar' 'team' 'teams' 'technical' 'technology' 'tell' 'term'\n",
      " 'terms' 'test' 'text' 'thank' 'thanks' 'theory' 'thing' 'things' 'think'\n",
      " 'thinking' 'thought' 'time' 'times' 'title' 'today' 'told' 'took' 'tools'\n",
      " 'toronto' 'total' 'trade' 'transfer' 'tried' 'trouble' 'true' 'trust'\n",
      " 'truth' 'trying' 'turkey' 'turkish' 'turks' 'turn' 'turned' 'type'\n",
      " 'types' 'uk' 'understand' 'understanding' 'unfortunately' 'unit' 'united'\n",
      " 'university' 'unix' 'unless' 'use' 'used' 'useful' 'usenet' 'user'\n",
      " 'users' 'uses' 'using' 'usually' 'value' 'values' 'various' 've'\n",
      " 'version' 'video' 'view' 'voice' 'volume' 'w7' 'wait' 'want' 'wanted'\n",
      " 'wants' 'war' 'washington' 'watch' 'water' 'way' 'ways' 'weapons' 'week'\n",
      " 'weeks' 'went' 'west' 'western' 'white' 'wide' 'widget' 'wife' 'willing'\n",
      " 'win' 'window' 'windows' 'wire' 'wish' 'wm' 'woman' 'women' 'wonder'\n",
      " 'wondering' 'word' 'words' 'work' 'worked' 'working' 'works' 'world'\n",
      " 'worse' 'worth' 'write' 'writing' 'written' 'wrong' 'wrote' 'x11r5'\n",
      " 'year' 'years' 'yes' 'york' 'young']\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out() # 단어 집합. 1,000개의 단어가 저장됨. \n",
    "# When sklearn.__version__ >= 1.0.x use following method\n",
    "\n",
    "print(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5))for i in topic.argsort()[:-n - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.2085), ('know', 0.19656), ('people', 0.1912), ('think', 0.17523), ('good', 0.14902)]\n",
      "Topic 2: [('thanks', 0.31338), ('windows', 0.27934), ('card', 0.17289), ('drive', 0.16141), ('mail', 0.14507)]\n",
      "Topic 3: [('game', 0.36553), ('team', 0.3133), ('year', 0.28465), ('games', 0.23048), ('season', 0.17026)]\n",
      "Topic 4: [('edu', 0.50341), ('thanks', 0.25409), ('mail', 0.1758), ('com', 0.11498), ('email', 0.11166)]\n",
      "Topic 5: [('edu', 0.49934), ('drive', 0.24972), ('com', 0.10645), ('sale', 0.10616), ('soon', 0.09199)]\n",
      "Topic 6: [('drive', 0.40102), ('thanks', 0.34667), ('know', 0.27592), ('scsi', 0.13765), ('mail', 0.11332)]\n",
      "Topic 7: [('chip', 0.21565), ('government', 0.20249), ('like', 0.17148), ('encryption', 0.14654), ('clipper', 0.14478)]\n",
      "Topic 8: [('like', 0.64668), ('edu', 0.31439), ('bike', 0.12683), ('know', 0.12403), ('think', 0.11547)]\n",
      "Topic 9: [('card', 0.3572), ('sale', 0.17543), ('00', 0.17496), ('video', 0.16994), ('good', 0.15574)]\n",
      "Topic 10: [('card', 0.45093), ('people', 0.35248), ('know', 0.26213), ('video', 0.20438), ('edu', 0.18018)]\n",
      "Topic 11: [('like', 0.53784), ('people', 0.29344), ('windows', 0.14585), ('drive', 0.14199), ('game', 0.138)]\n",
      "Topic 12: [('think', 0.73098), ('thanks', 0.19253), ('people', 0.12027), ('mail', 0.11436), ('good', 0.11292)]\n",
      "Topic 13: [('know', 0.21625), ('chip', 0.21482), ('jesus', 0.19227), ('game', 0.1597), ('think', 0.15475)]\n",
      "Topic 14: [('know', 0.33277), ('good', 0.31777), ('people', 0.22055), ('windows', 0.21856), ('file', 0.20588)]\n",
      "Topic 15: [('good', 0.38909), ('chip', 0.22814), ('people', 0.15098), ('clipper', 0.14368), ('encryption', 0.14025)]\n",
      "Topic 16: [('com', 0.6569), ('ve', 0.34058), ('people', 0.20544), ('seen', 0.0979), ('list', 0.09405)]\n",
      "Topic 17: [('ve', 0.38594), ('space', 0.30952), ('people', 0.28754), ('seen', 0.13734), ('heard', 0.13676)]\n",
      "Topic 18: [('good', 0.32101), ('israel', 0.25517), ('card', 0.24702), ('com', 0.1984), ('israeli', 0.13667)]\n",
      "Topic 19: [('ve', 0.54441), ('israel', 0.20771), ('seen', 0.17641), ('00', 0.14313), ('heard', 0.13857)]\n",
      "Topic 20: [('time', 0.34123), ('bike', 0.28176), ('right', 0.25465), ('file', 0.20011), ('need', 0.19803)]\n"
     ]
    }
   ],
   "source": [
    "get_topics(svd_model.components_, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "T3Q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
